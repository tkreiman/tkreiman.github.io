<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Transformers Discover Molecular Structure Without Graph Priors">
  <meta name="keywords" content="graph-free, force field, machine learning, pre-training, scaling laws">
  <meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no">
  <title>Transformers Adaptively Learn Molecular Structure Without Graph Priors</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <!--<link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">-->
  <!--<link rel="stylesheet" href="./static/css/fontawesome.all.min.css">-->
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">
  
  <style>
    body {
      font-size: 1.5rem; /* Increase base font size by 20% */
    }
  </style>

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <!-- <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>-->
  <script src="./static/js/index.js"></script>
  <script src="./static/js/ngl.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://tkreiman.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Transformers Discover Molecular Structure Without Graph Priors</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://tkreiman.github.io">Tobias Kreiman</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://yutongbai.com/">Yutong Bai</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://www.linkedin.com/in/fadi-atieh/">Fadi Atieh</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://elizabethweaver.me/index.html">Elizabeth Weaver</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://people.eecs.berkeley.edu/~ericqu/">Eric Qu</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://a1k12.github.io/">Aditi S. Krishnapriyan</a><sup>1</sup></span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>UC Berkeley</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper Coming Soon</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link.
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code coming soon</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Overview</h2>
      <div class="columns">
        <div class="column">
           <div class="content has-text-justified">
            <br>
            
            <b>Problem:</b> Graph Neural Networks (GNNs) are the dominant architecture for molecular machine learning (ML) but depend on <b>predefined graphs</b>, which limit expressivity and slow inference.
            <br>
            <br>
            <b>Investigation:</b> We test if a model trained directly on Cartesian coordinates—<b>without predefined graphs or physical priors</b>—can learn molecular energies and forces, gaining the flexibility to discover interactions from data and opening the door to leverage scaling laws and architectures from broader deep learning.
            <br>
            <br>
            <b>Approach:</b> We demonstrate that a graph-free Transformer, trained under equal compute, achieves competitive accuracy with a state-of-the-art equivariant GNN on OMol25. We find that it learns physically consistent distance-aware patterns without hard-coded priors and exhibits predictable gains with scale—showing that <b>many benefits of GNNs can emerge adaptively in Transformers.</b>
          </div>
        </div>
      </div>
  </section>


  <section class="section">
    <div class="container is-max-desktop">

      <h2 class="title is-3">What Intermolecular Structure Do Graph-Free Transformers Discover?</h2>
      <br>
      <p>
         We visualize the attention scores from the 4th layer of our Transformer for 3 molecules from the OMol25 validation dataset. <span style="font-weight:bold">Click on an atom</span> to see the learned attention scores for that atom. Red is <span style="color:red; font-weight:bold">more attention</span>,
         blue is <span style="color:blue; font-weight:bold">less attention</span>.
      </p>
       
       <!-- Interactive Molecule Viewer -->
       <div style="margin: 20px 0; padding: 20px; background: #f8f9fa; border-radius: 12px; border: 2px solid #e9ecef; box-shadow: 0 2px 4px rgba(0,0,0,0.1);">
         <div style="display: flex; align-items: center; gap: 15px; margin-bottom: 15px; flex-wrap: wrap;">
           <h4 style="margin: 0; color: #333; font-size: 20px;">Select Molecule:</h4>
           <select id="molecule-selector" style="padding: 12px 18px; border: 2px solid #007bff; border-radius: 6px; font-size: 18px; background: white; cursor: pointer; transition: all 0.2s; min-height: 44px; -webkit-appearance: none; -moz-appearance: none; appearance: none;">
             <option value="1">Molecule 1</option>
             <option value="2">Molecule 2</option>
             <option value="3">Molecule 3</option>
           </select>
         </div>
         <div style="border-top: 1px solid #dee2e6; padding-top: 15px;">
           <h4 style="margin: 0 0 10px 0; color: #333; font-size: 20px;">Chemical Formula: <span id="chemical-formula" style="font-weight: bold; color: #007bff;">Loading...</span></h4>
           <div id="atom-legend" style="display: flex; flex-wrap: wrap; gap: 10px; margin-top: 10px;">
             <!-- Legend will be populated by JavaScript -->
           </div>
         </div>
         <div style="border-top: 1px solid #dee2e6; padding-top: 15px; margin-top: 15px;">
           <div style="background:#eaeaea; border-radius:12px; padding:12px;">
             <h4 style="margin: 0 0 10px 0; color: #333; font-size: 20px;">Click Atoms to See Learned Attention Scores (Desktop Only):</h4>  
             <div id="viewport" style="width:100%; height:400px;"></div>

           </div>
         </div>
       </div>
       <p>The Transformer naturally <b>learns attention patterns that are inversely proportional to interatomic distance.</b></p>
  <script>
    document.addEventListener("DOMContentLoaded", function () {
      var stage = new NGL.Stage("viewport", { backgroundColor: "white" });
      var comp = null;
      var baseRep = null;      // single representation with attention-based colors
      var attnMatrix = null;   // 2D array [N x N]
      var currentWeights = null; // 1D array length N
      var autoRotate = true;   // enable automatic rotation
      var isUserInteracting = false; // track if user is interacting
      var currentMolecule = "1"; // current selected molecule

      // Red -> Blue color scheme by attention (red=low, blue=high)
      var schemeId = NGL.ColormakerRegistry.addScheme(function () {
        this.atomColor = function (atom) {
          if (!currentWeights || !atom || atom.index >= currentWeights.length) return 0x808080;
          var v = currentWeights[atom.index] || 0.0; // assume already normalized [0,1]
          // v=0 => red (255,0,0), v=1 => blue (0,0,255)
          var r = Math.round(255 * (1 - v));
          var g = 0;
          var b = Math.round(255 * v);
          return (r << 16) | (g << 8) | b;
        };
      });

      function clearHeatmap() { /* single rep, nothing to clear */ }

      // Function to load a specific molecule
      function loadMolecule(moleculeNum) {
        currentMolecule = moleculeNum;
        var pdbFile = "./atoms_" + moleculeNum + ".pdb";
        var attnFile = "./attn_matrix_" + moleculeNum + ".json";
        
        // Clear existing structure
        if (comp) {
          stage.removeComponent(comp);
        }
        
        // Load new structure
        stage.loadFile(pdbFile, { defaultRepresentation: false }).then(function (c) {
          comp = c;
          
          // Calculate chemical formula and create atom legend
          var atomCounts = {};
          for (var i = 0; i < comp.structure.atomCount; i++) {
            var atom = comp.structure.getAtomProxy(i);
            var element = atom.element;
            atomCounts[element] = (atomCounts[element] || 0) + 1;
          }
          
          // Generate properly formatted chemical formula with subscripts
          var elements = Object.keys(atomCounts).sort();
          var formulaHTML = '';
          for (var j = 0; j < elements.length; j++) {
            var elem = elements[j];
            var count = atomCounts[elem];
            // Ensure proper capitalization: first letter uppercase, rest lowercase
            var properElement = elem.charAt(0).toUpperCase() + elem.slice(1).toLowerCase();
            formulaHTML += properElement;
            if (count > 1) formulaHTML += '<sub>' + count + '</sub>';
          }
          document.getElementById('chemical-formula').innerHTML = formulaHTML;
          
          // Create atom legend
          var legendDiv = document.getElementById('atom-legend');
          legendDiv.innerHTML = ''; // Clear existing legend
          for (var k = 0; k < elements.length; k++) {
            var element = elements[k];
            var count = atomCounts[element];
            
            var legendItem = document.createElement('div');
            legendItem.style.cssText = 'display: flex; align-items: center; gap: 5px; padding: 5px 10px; background: white; border-radius: 4px; border: 1px solid #ddd;';
            
            var colorBox = document.createElement('div');
            // Use standard element colors
            var elementColors = {
              'H': '#FFFFFF', 'C': '#909090', 'N': '#3050F8', 'O': '#FF0D0D',
              'F': '#90E050', 'P': '#FF8000', 'S': '#FFFF30', 'Cl': '#1FF01F',
              'Br': '#A62929', 'I': '#940094', 'He': '#D9FFFF', 'Li': '#CC80FF',
              'Be': '#C2FF00', 'B': '#FFB5B5', 'Ne': '#B3E3F5', 'Na': '#AB5CF2', 'Mg': '#8AFF00',
              'Al': '#BFA6A6', 'Si': '#F0C8A0', 'Ar': '#80D1E3', 'K': '#8F40D4',
              'Ca': '#3DFF00', 'Ti': '#BFC2C7', 'Cr': '#8A99C7', 'Mn': '#9C7AC7',
              'Fe': '#E06633', 'Co': '#F090A0', 'Ni': '#50D050', 'Cu': '#C88033',
            'Zn': '#7D80B0', 'Ga': '#C28F8F', 'Ge': '#668F8F', 'As': '#BD80E3',
            'Se': '#FFA100', 'Kr': '#5CB8D1', 'Rb': '#702EB0', 'Sr': '#00FF00',
            'Y': '#94FFFF', 'Zr': '#94E0E0', 'Nb': '#73C2C9', 'Mo': '#54B5B5'
          };
          // Use proper capitalization for color lookup
            var properElement = element.charAt(0).toUpperCase() + element.slice(1).toLowerCase();
            var color = elementColors[properElement] || '#808080';
            colorBox.style.cssText = 'width: 16px; height: 16px; border-radius: 50%; background-color: ' + color + '; border: 1px solid #333;';
            
            var label = document.createElement('span');
            label.textContent = properElement + (count > 1 ? ' (' + count + ')' : '');
            label.style.fontWeight = 'bold';
            
            legendItem.appendChild(colorBox);
            legendItem.appendChild(label);
            legendDiv.appendChild(legendItem);
          }
          
          // Create single ball+stick representation
          baseRep = comp.addRepresentation("ball+stick", {
            color: "element",
            opacity: 0.9,
            radiusType: "vdw",
            radiusScale: 0.175,
            bondScale: 0.8
          });
          stage.autoView();

          // Enable automatic rotation using NGL's built-in feature
          stage.setSpin(true);           // start spinning
          stage.setSpin([0, 1, 0], 0.5);  // axis [x,y,z], speed in revolutions/sec

          // Function to handle atom selection (used by both click and touch)
          function handleAtomSelection(pickingProxy) {
            console.log("handleAtomSelection called", pickingProxy);
            if (!pickingProxy || !pickingProxy.atom || !comp || !baseRep || !attnMatrix) {
              console.log("Empty space clicked - resetting colors");
              // Clicked on empty space - reset to element colors
              currentWeights = null;
              comp.removeAllRepresentations();
              baseRep = comp.addRepresentation("ball+stick", {
                color: "element",
                opacity: 0.9,
                radiusType: "vdw",
                radiusScale: 0.175,
                bondScale: 0.8
              });
              return;
            }
            
            var colIdx = pickingProxy.atom.index;
            console.log("Atom clicked - index:", colIdx, "element:", pickingProxy.atom.element);
            var n = attnMatrix.length;
            if (colIdx < 0 || colIdx >= n) return;
            
            // Build column vector from attention matrix
            var vec = new Array(n);
            for (var j = 0; j < n; j++) {
              var row = attnMatrix[j];
              if (!Array.isArray(row) || row.length !== n) return;
              vec[j] = Number(row[colIdx]) || 0.0;
            }
            currentWeights = vec;
            console.log("Updated attention weights for atom", colIdx);
            
            // Clear all representations first to avoid overlapping
            comp.removeAllRepresentations();
            baseRep = comp.addRepresentation("ball+stick", {
              color: schemeId,
              opacity: 0.9,
              radiusType: "vdw",
              radiusScale: 0.175,
              bondScale: 0.8
            });
          }

          // Override default left-click behavior (which centers view) to recolor by attention
          try { stage.mouseControls.remove("clickPick-left"); } catch (e) {}
          stage.mouseControls.add("clickPick-left", function (stg, pickingProxy) {
            console.log("Mouse click detected");
            handleAtomSelection(pickingProxy);
          });

          // Add touch support using the same approach as mouse clicks
          try { stage.mouseControls.remove("touchPick"); } catch (e) {}
          stage.mouseControls.add("touchPick", function (stg, pickingProxy) {
            console.log("Touch pick detected");
            handleAtomSelection(pickingProxy);
          });
          
          // Load attention matrix JSON
          fetch(attnFile)
            .then(function (r) { if (!r.ok) throw new Error("Failed to load " + attnFile); return r.json(); })
            .then(function (mat) {
              if (!Array.isArray(mat) || mat.length === 0 || !Array.isArray(mat[0])) return;
              attnMatrix = mat;
            })
            .catch(function (err) { console.error(err); });
        });
      }

      // Add dropdown event listener
      document.getElementById('molecule-selector').addEventListener('change', function() {
        loadMolecule(this.value);
      });

      // Load initial molecule
      loadMolecule("1");
    });
  </script>
</div>

<div class="container is-max-desktop">
  <img id="teaser" src="./images/molecule_fragments.gif" alt="Molecule Fragments" width="100%">
 </div>

 <div class="container is-max-desktop">
 <p>Since the Transformer has no explicit hardcoded graph priors, the Transformer learns <b>adaptive attention patterns</b>, like an effective radius cutoff that increases when atoms become farther apart. In contrast, <b>GNNs use a fixed graph construction algorithm</b> that does not adapt to atomic environments.</p>
</div>

</section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">What Can Graph-Free Transformers Learn?</h2>
      <div class="columns">
        <div class="column">

          <h3 class="title is-4">Comparison to an Equivariant GNN on OMol25</h3>
          
          <p>We compare the energy and force mean absolute errors of a graph-free Transformer to a state-of-the-art equivariant GNN on the OMol25 dataset. <b>Under an equal training compute budget, the Transformer achieves competitive accuracy with the GNN.</b> Given that the model achieves competitive accuracy with the GNN, it provides a good starting point to analyze the interatomic interactions that are learned without graph priors.</p>

          <div style="overflow-x:auto; margin: 1em 0;">
            <table style="border-collapse: collapse; width: 100%; font-family: sans-serif; text-align: center; border-top: 2px solid black; border-bottom: 2px solid black;">
              <thead>
                <tr style="border-bottom: 2px solid black;">
                  <th style="text-align:left; padding: 6px;">Model</th>
                  <th>FLOPs</th>
                  <th>Forward Latency (ms)</th>
                  <th>Training Speed (atoms/sec)</th>
                  <th>Energy MAE (meV)</th>
                  <th>Forces MAE (meV/Å)</th>
                </tr>
              </thead>
              <tbody>
                <tr style="border-bottom: 1px solid #ccc;">
                  <td style="text-align:left; padding: 6px;">eSEN-sm-d 6M</td>
                  <td>O(10<sup>20</sup>)</td>
                  <td>26.3</td>
                  <td>32k+</td>
                  <td>129.77</td>
                  <td>13.01</td>
                </tr>
                <tr>
                  <td style="text-align:left; padding: 6px;">Transformer 1B (Ours)</td>
                  <td>8.5×10<sup>19</sup></td>
                  <td>17.2</td>
                  <td>42k+</td>
                  <td>117.99</td>
                  <td>18.35</td>
                </tr>
              </tbody>
            </table>
          </div>
          
          
          <!-- <div class="container is-max-desktop">
            <img id="teaser" src="./images/omol_table.png" alt="OMol25 Table" width="100%">
           </div> -->

          <h3 class="title is-4">Scaling Laws</h3>
          <p>Using an unmodified Transformer allows us to leverage mature software and hardware frameworks, enabling us to train a model up to 1B parameters. <b>We find that the Transformer exhibits consistent and predictable gains with respect to scaling training resources</b>, consistent with empirical scaling laws observed in other domains.</p>
           <div class="columns">
            <div class="column is-six-sixteenths">
              <img src="images/model_scaling_omol.png" alt="Model Scaling OMol25" style="width:100%">
            </div>
            <div class="column is-ten-sixteenths">
              <img src="images/ft_flops.png" alt="FT FLOPS" style="width:100%">
            </div>
          </div>

          <h3 class="title is-4">Attention Patterns</h3>

          <p>The graph-free Transformer naturally learns physically consistent attention patterns from data, such as <b>attention scores that decay inversely with interatomic distance.</b> The absence of the hard-coded graph priors allows the Transformer to <b>learn adaptive attention patterns</b>, such as an effective radius cutoff that increases when atoms become farther apart.</p>
          <div class="columns">
            <div class="column is-six-sixteenths">
              <img src="images/attn_vs_dist_nine_layers.png" alt="Attn vs Dist Nine Layers" style="width:100%">
            </div>
            <div class="column is-ten-sixteenths">
              <img src="images/attn_effective_rad_square.png" alt="Attn Effective Rad Square" style="width:90%">
            </div>
          </div>

        </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">How did we do it?</h2>
      <div class="content has-text-justified">
      <p>
      We simply train an unmodified Transformer on the OMol25 dataset, with the only modification being the addition of a continuous input stream in addition to discrete tokens. The attention mechanism remains completely unchanged. We leverage existing software and hardware for Transformers to efficiently train our models.
      
        </p>

        <div class="container is-max-desktop">
          <img id="teaser" src="./images/teaser.png" alt="Teaser Diagram" width="100%">
         </div>
      </div>
    </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">What does this mean for the future of molecular machine learning?</h2>
      <div class="content has-text-justified">
        <p>
      Our findings suggest that <b>Transformers can learn many of the graph-based inductive biases typically built into current ML models for chemistry—while doing so more flexibly.</b> We hope these findings point towards a standardized, widely applicable architecture for molecular modeling that draws on insights from the broader deep learning community.
      <br>
      <br>
      For more details, check out <a href="" target="_blank">our paper!</a>
        </p>
      </div>
    </div>
  </section>

  </section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <pre><code></code></pre>
  </div> 
</section>


<footer class="footer">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
            under a 
            <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>
          