<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <meta name="description"
        content="Understanding and Mitigating Distribution Shifts for Machine Learning Force Fields.">
  <meta name="keywords" content="distribution, shifts, force, field, machine learning">
  <meta name="viewport" content="width=device-width, initial-scale=1">
  <title>Understanding and Mitigating Distribution Shifts for Machine Learning Force Fields</title>

  <!-- Global site tag (gtag.js) - Google Analytics -->
  <!-- <script async src="https://www.googletagmanager.com/gtag/js?id=G-PYVRSFMDRL"></script>
  <script>
    window.dataLayer = window.dataLayer || [];

    function gtag() {
      dataLayer.push(arguments);
    }

    gtag('js', new Date());

    gtag('config', 'G-PYVRSFMDRL');
  </script> -->

  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
        rel="stylesheet">

  <link rel="stylesheet" href="./static/css/bulma.min.css">
  <link rel="stylesheet" href="./static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="./static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="./static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
        href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="./static/css/index.css">
  <link rel="icon" href="./static/images/favicon.svg">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script defer src="./static/js/fontawesome.all.min.js"></script>
  <script src="./static/js/bulma-carousel.min.js"></script>
  <script src="./static/js/bulma-slider.min.js"></script>
  <script src="./static/js/index.js"></script>
</head>
<body>

<nav class="navbar" role="navigation" aria-label="main navigation">
  <div class="navbar-brand">
    <a role="button" class="navbar-burger" aria-label="menu" aria-expanded="false">
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
      <span aria-hidden="true"></span>
    </a>
  </div>
  <div class="navbar-menu">
    <div class="navbar-start" style="flex-grow: 1; justify-content: center;">
      <a class="navbar-item" href="https://tkreiman.github.io">
      <span class="icon">
          <i class="fas fa-home"></i>
      </span>
      </a>

      <!-- <div class="navbar-item has-dropdown is-hoverable">
        <a class="navbar-link">
          More Research
        </a>
        <div class="navbar-dropdown">
          <a class="navbar-item" href="https://hypernerf.github.io">
            HyperNeRF
          </a>
          <a class="navbar-item" href="https://nerfies.github.io">
            Nerfies
          </a>
          <a class="navbar-item" href="https://latentfusion.github.io">
            LatentFusion
          </a>
          <a class="navbar-item" href="https://photoshape.github.io">
            PhotoShape
          </a>
        </div>
      </div> -->
    </div>

  </div>
</nav>


<section class="hero">
  <div class="hero-body">
    <div class="container is-max-desktop">
      <div class="columns is-centered">
        <div class="column has-text-centered">
          <h1 class="title is-1 publication-title">Understanding And Mitigating Distribution Shifts for MLFFs</h1>
          <div class="is-size-5 publication-authors">
            <span class="author-block">
              <a href="https://tkreiman.github.io">Tobias Kreiman</a><sup>1</sup>,</span>
            <span class="author-block">
              <a href="https://a1k12.github.io/">Aditi Krishnapriyan</a><sup>1</sup>,</span>
          </div>

          <div class="is-size-5 publication-authors">
            <span class="author-block"><sup>1</sup>UC Berkeley,</span>
          </div>

          <div class="column has-text-centered">
            <div class="publication-links">
              <!-- PDF Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fas fa-file-pdf"></i>
                  </span>
                  <span>Paper Coming Soon</span>
                </a>
              </span>
              <!-- <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
              <!-- Video Link.
              <span class="link-block">
                <a href="https://www.youtube.com/watch?v=MrKrnHhk8IA"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-youtube"></i>
                  </span>
                  <span>Video</span>
                </a>
              </span> -->
              <!-- Code Link. -->
              <span class="link-block">
                <a href=""
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="fab fa-github"></i>
                  </span>
                  <span>Code Coming Soon</span>
                  </a>
              </span>
              <!-- Dataset Link. -->
              <!-- <span class="link-block">
                <a href="https://github.com/google/nerfies/releases/tag/0.1"
                   class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                      <i class="far fa-images"></i>
                  </span>
                  <span>Data</span>
                  </a> -->
            </div>

          </div>
        </div>
      </div>
    </div>
  </div>
</section>

<!-- <section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video id="teaser" autoplay muted loop playsinline height="100%">
        <source src="./static/videos/teaser.mp4"
                type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        <span class="dnerf">Nerfies</span> turns selfie videos from your phone into
        free-viewpoint
        portraits.
      </h2>
    </div>
  </div>
</section> -->

<section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Overview</h2>
      <div class="columns">
        <div class="column">
          <div class="container is-max-desktop">
            <img id="teaser" src="./images/distribution_shifts.png" alt="Distribution Shifts Diagram" width="100%">
           </div>
           <div class="content has-text-justified">
            <br>
            <!-- <p>
              Given the computational expense of ab initio simulations for all chemical spaces of interest, there has been a push to train larger and more accurate machine learning force fields (MLFFs), designed to work well across many different systems. 
              A goal here is developing models with general representations that accurately capture diverse chemistries and eliminate the need to recollect data and retrain a model for each new system. 
              To determine which systems an MLFF can accurately describe and to assess the reliability of its predictions, it is important to understand how MLFFs generalize beyond their training distributions. 
              This understanding is essential for applying MLFFs to new and diverse chemical spaces, ensuring that they perform well not only on the data they were trained on, but also on unseen, potentially more complex systems.
            </p> -->
            <p>
              We conduct an in-depth exploration to identify and understand distribution shifts for machine learning force fields (MLFFs). 
              On example chemical datasets, we find that <b>state-of-the-art models struggle with common distribution shifts.</b>
              We demonstrate that there are multiple reasons that this is the case, including challenges associated with poorly-connected graphs and learning unregularized representations, evidenced by jagged predicted potential energy surfaces for out-of-distribution systems.    
            </p>
            <p>
              Building off of our observations, we propose two paths forward that <b>take initial steps at mitigating distribution shifts for MLFFs through test-time refinement strategies.</b>
              We extensively test our approaches and show that our test-time refinement strategies are effective in mitigating distribution shifts for MLFFs.
              Our experiments establish clear benchmarks that highlight ambitious but important generalization goals for the next generation of MLFFs.
              Additionally, the success of our test-time refinement strategies provides insights into why MLFFs are susceptible to distribution shifts, namely overfitting and poorly regularized representations (as opposed to only poor data or weak architectures).
              This suggests that <b>while MLFFs are expressive enough to model diverse chemical spaces, they are not being effectively trained to do so.</b>
            </p>
          </div>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Understanding Failure Modes for Current Chemical Foundation Models</h2>
      <div class="columns">
        <div class="column">
          <div class="container is-max-desktop">
            <img id="teaser" src="./images/foundation_bad_wide.png" alt="Distribution Shifts Diagram" width="100%">
           </div>
           <div class="content has-text-justified">
            <br>
            <p>
              We first formalize three criteria for identifying distribution shifts based on the features, labels, and graph structures in chemical datasets. 
              This categorization provides a framework for understanding the types of distribution shifts an MLFF may encounter:
              <ol>
                <li><b>Atomic Features</b>: Distribution shifts in atomic features are the most apparent and detrimental to the performance of current state-of-the-art models. This may involve encountering a molecule with a new element at test time that was not present during training.</li>
                <li><b>Force Norms</b>: An MLFF may also encounter a distribution shift in the force labels it predicts. A model trained on structures close to equilibrium, with low force magnitudes, might be tested on a structure with higher force norms.</li>
                <li><b>Graph Structure / Connectivity</b>: Since many MLFFs are implemented as graph neural networks (GNNs), they may encounter distribution shifts in the graph structure of the test data. We refer to these as connectivity distribution shifts. We study these distribution shfits by analyzing the spectrum of the Laplacian for molecular graphs.</li>
              </ol>
              We contextualize the aforementioned distribution shifts by considering four large foundation models: MACE-OFF, MACE-MP, EquiformerV2, and JMP.
              These models represent four of the largest open-source MLFFs to date, and they have been trained on some of the most extensive datasets available. We focus on these models since their scale is designed for tackling broad chemical spaces.
              <b>Despite their scale, these models struggle predictiably when encountering distribution shifts along the three criteria we have identified. </b>
              This deterioration is severe (often by an order of magnitude) and consistent across models and datasets. 
            </p>
          </div>
        </div>
      </div>
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Mitigation Strategies for Distribution Shifts</h2>
      <div class="columns">
        <div class="column">
          <!-- <p>
            Based on the generalization challenges for the large foundation models, we hypothesize that many MLFFs are severely overfitting to the training data, resulting in a failure to learn generalizable representations.
            To test this hypothesis, we develop two test-time refinement strategies that also mitigate distribution shifts.
            The effectiveness of these test-time refinement strategies, validated through extensive experiments (which are highlighted below), may indicate that MLFFs are currently poorly regularized and overfit to graph structures seen during training, hindering broader generalization.
          </p>
          <br> -->
          <p>
            First, we study how MLFFs tend to overfit to the (typically regular and well connected) graph structures encountered during training by looking at the graph Laplacian spectrum of chemical graphs.
            At test time, we can then identify when an MLFF encounters a graph with a Laplacian eigenvalue distribution that significantly differs from the training graphs.
            Since the edges in chemical graphs are typically generated by a radius graph, <b>we refine the radius cutoff at test time to update testing graphs to more closely resemble the training structures, mitigating the connectivity distribution shift.</b>
            We call this test-time radius refinement (RR).
            This refinement method addresses the source of connectivity distribution shifts and serves as a simple and effective initial strategy for handling new connectivities.   
          </p>
          <br>
          <div class="container is-max-desktop" style="text-align: center;">
            <img id="teaser" src="./images/radius_refinement_2.png" alt="Distribution Shifts Diagram" width="75%">
          </div>
           <div class="content has-text-justified">
            <br>
            <p>
              We further hypothesize that the current supervised training procedure for MLFFs can lead to overfitting, leading to poor representations for out-of-distribution systems and jagged potential energy landscape predictions.
              To address this, we propose introducing inductive biases through improved training and inference strategies.
              We represent these inductive biases as cheap priors, such as classical force fields or simple ML models.
              Following previous test-time training (TTT) works, we pre-train (or joint-train) our model to learn features from the prior. 
              At test-time, we can then take gradient steps on the prior targets since the prior is cheap to evaluate.
              <b>Taking gradient steps on the prior incorporates inductive biases about the out-of-distribution samples into the model, regularizing the energy landscape and helping the model generalize.</b>
            </p>
          </div>
          <div class="container is-max-desktop" style="text-align: center;">
            <img id="teaser" src="./images/ttt+pes.drawio.png" alt="Distribution Shifts Diagram" width="100%">
           </div>
           <!-- <div class="content has-text-justified">
            <br>
            <p>
              We mitigate 2.
            </p>
          </div> -->
        </div>
      </div>
  </section>

  

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">Practical Implications</h2>
      
      <p>
        We conduct experiments on chemical datasets to both identify the presence of distribution shifts and evaluate the effectiveness of our test-time refinement strategies to mitigate these shifts.
        We first explore the MACE-OFF model in more detail, investigating distribution shifts from the SPICE dataset to the SPICEv2 dataset.
        We observe that despite its scale, MACE-OFF experiences force norm, connectivity, and element distribution shifts when evaluated on 10k new molecules from SPICEv2.
        <b>Any deviation from the training distribution, shown in gray, predictably results in an increase in force error.</b>
      </p>
      <br>
      <p>
        We also evaluate the effectiveness of our test-time refinement strategies in mitigating these distribution shifts by implementing our RR approach on top of MACE-OFF and applying TTT to a GemNet-T model we trained.
        TTT reduces errors for GemNet-T on out-of-distribution force norms and connectivities, and also helps decrease errors for the new systems that are closer to the training distribution.
        Our test-time radius refinement (RR) technique applied to MACE-OFF effectively mitigates connectivity errors at minimal computational cost.
        <b>These improvements result in hundreds of molecular systems getting their errors brought down well below 25 meV / Å (shown below).</b>
      </p>
      <br>
      <div class="columns">
        <div class="column is-six-sixteenths">
          <img src="images/connect_100p.png" alt="connect" style="width:90%">
        </div>
        <div class="column is-ten-sixteenths">
          <img src="images/force_norm_100p.png" alt="force_norm" style="width:100%">
        </div>
      </div>

      <p>
        We establish an extreme distribution shift benchmark to evaluate the generalization ability of MLFFs on the MD17 dataset.
        We train a GemNet-T moodel on 3 molecules from the MD17 dataset (benzene, aspirin, and uracil), and we evaluate whether it can simulate two new molecules (naphthalene and toluene) which were unseen during training.
        Without TTT, the GemNet-dT model trained only on aspirin, benzene, and uracil is unable to stably simulate the new molecules and poorly reproduces observables.
        <b>TTT enables stable simulations of the unseen molecules that accurately reproduce the distribution of interatomic distances h(r).</b>
        Furthermore, TTT provides a better starting point for fine-tuning, <b>decreasing the amount of data needed to reach the in-distribution performance by more than 20x.</b>
      </p>
      <br>
      <div class="columns">
        <div class="column is-half">
          <img src="images/naph_hr.png" alt="naph" style="width:100%">
        </div>
        <div class="column is-half">
          <img src="images/tolu_hr.png" alt="tolu" style="width:100%">
        </div>
      </div>



      <div class="columns">
        <div class="column is-three-fifths">
          <img src="images/ttt_binned.png" alt="binned" style="width:100%">
        </div>
        <div class="column is-half">
          <img src="images/ttt_finetune.png" alt="ft" style="width:100%">
        </div>
      </div>

      <!-- <div class="content has-text-justified">
        <br>
        <p></p>
      </div> -->
  </section>

  <section class="section">
    <div class="container is-max-desktop">
      <h2 class="title is-3">What does this mean for the field of MLFFs?</h2>
      <div class="columns">
        <div class="column">
          <!-- <div class="container is-max-desktop">
            <img id="teaser" src="./images/foundation_bad_wide.png" alt="Distribution Shifts Diagram" width="100%">
           </div> -->
           <div class="content has-text-justified">
            <br>
            <p>
              Our results reveal interesting insights into the current state of MLFFs which we summarize below:
              <ol>
                <li>State-of-the-art MLFFs, even when trained on large datasets, suffer from <b>predictable</b> performance degradation due to distribution shifts</li>
                <li>Common distribution shifts can be classified into element, force norm, and connectivity distribution shifts, and <b>we show how to diagnose them using spectral graph theory and cheap priors.</b></li>
                <li><b>Test-time refinement methods represent initial steps in mitigating these distribution shifts</b>, showing promising results in modeling and simulating systems outside of the training distribution.</li>
                <li>The success of these methods provides insights into how MLFFs overfit, suggesting that <b>while MLFFs are expressive enough to model diverse chemical spaces, they are not being effectively trained to do so.</b></li>
              </ol>
              More generally, given the community's interest in training larger and more accurate MLFFs designed to work well across many different systems, it is important to understand how MLFFs generalize beyond their training distributions.
              This understanding is essential for applying MLFFs to new and diverse chemical spaces, ensuring that they perform well not only on the data they were trained on, but also on unseen, potentially more complex systems.
              Our work suggests that training strateiges, alongside archtecture and data innovations, might play an important role developing the next generation of MLFFs. 
            </p>
            <br>
            <p>
              For more details, check out our paper!
            </p>
          </div>
        </div>
      </div>
  </section>

<section class="section" id="BibTeX">
  <div class="container is-max-desktop content">
    <h2 class="title">BibTeX</h2>
    <!-- <pre><code>@article{park2021nerfies,
  author    = {Park, Keunhong and Sinha, Utkarsh and Barron, Jonathan T. and Bouaziz, Sofien and Goldman, Dan B and Seitz, Steven M. and Martin-Brualla, Ricardo},
  title     = {Nerfies: Deformable Neural Radiance Fields},
  journal   = {ICCV},
  year      = {2021},
}</code></pre>
  </div> -->
</section>


<footer class="footer">
  <div class="container is-max-desktop">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">
          <p>
            Website template borrowed from <a href="https://github.com/nerfies/nerfies.github.io">NeRFies</a>
            under a 
            <a rel="license"
                                                href="http://creativecommons.org/licenses/by-sa/4.0/">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>
        </div>
      </div>
    </div>
  </div>
</footer>

</body>
</html>